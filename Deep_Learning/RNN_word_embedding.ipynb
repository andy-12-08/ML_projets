{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "80a4c9c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tensorflow.keras.preprocessing.text import one_hot\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import Flatten\n",
    "from tensorflow.keras.layers import Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "f1d692be",
   "metadata": {},
   "outputs": [],
   "source": [
    "## customer reviews\n",
    "\n",
    "reviews = ['nice food',\n",
    "          'amazing restaurant',\n",
    "          'too good',\n",
    "          'will go again',\n",
    "          'horrible food',\n",
    "          'never go there',\n",
    "          'poor service',\n",
    "          'poor quality',\n",
    "          'needs improvement']\n",
    "\n",
    "## reviews labels (class labels)\n",
    "sentiment = np.array([1,1,1,1,0,0,0,0,0,])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "4783b438",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[9, 27]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### using one_hot encoding in keras to assign random unique\n",
    "### number to each word, within a specified range (vocabulary size)\n",
    "\n",
    "one_hot('amazing restaurant', 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "def37847",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[39, 23],\n",
       " [12, 44],\n",
       " [27, 25],\n",
       " [31, 47, 22],\n",
       " [48, 23],\n",
       " [2, 47, 31],\n",
       " [2, 41],\n",
       " [2, 36],\n",
       " [30, 39]]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Now we want to encode all the reviews\n",
    "vocab_size = 50\n",
    "## the random numbers assigned should be in range 0-30\n",
    "\n",
    "encoded_reviews = [one_hot(d, vocab_size) for d in reviews]  # a simple list comprehension\n",
    "encoded_reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "43e4ec7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[39 23  0]\n",
      " [12 44  0]\n",
      " [27 25  0]\n",
      " [31 47 22]\n",
      " [48 23  0]\n",
      " [ 2 47 31]\n",
      " [ 2 41  0]\n",
      " [ 2 36  0]\n",
      " [30 39  0]]\n"
     ]
    }
   ],
   "source": [
    "### we need a maximum sentence size \n",
    "### for this we need padding (we need to pad the sentence)\n",
    "### because some sentences are 3 words long, some are 2\n",
    "### for the 2-word sentences, we need to pad and append 0\n",
    "\n",
    "max_length = 3\n",
    "\n",
    "## using a method called pad_sequences in tensorflow keras\n",
    "## supply all the encoded reviews into pad_sequences\n",
    "## padding = 'post' means, pad the reviews towards the end\n",
    "padded_reviews = pad_sequences(encoded_reviews, maxlen=max_length, padding='post')\n",
    "print(padded_reviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "983f8dd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Now we want our embedding vector size to be 4\n",
    "embedded_vector_size = 4\n",
    "\n",
    "## next we create our model\n",
    "model = Sequential()\n",
    "\n",
    "## the first layer will be the embedding layer\n",
    "## using our Embedding class, which was imported\n",
    "## this takes vocab_size, embedded_vector_size, input_length, and we give it a name so we can use it later\n",
    "model.add(Embedding(vocab_size,embedded_vector_size, input_length=max_length, name='embedding'))\n",
    "\n",
    "# next is to add your flatten layer \n",
    "model.add(Flatten())\n",
    "\n",
    "# next is a dense layer with one neuron and an activation function\n",
    "model.add(Dense(1, activation='sigmoid'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "05223db5",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = padded_reviews # training samples\n",
    "y = sentiment     # labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "02b8390d",
   "metadata": {},
   "outputs": [],
   "source": [
    "### we end up using adam as an optimizer and binary_crossentropy because the output is either 1 or 0\n",
    "### the review is either positive or negative\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "4a5acf4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       (None, 3, 4)              200       \n",
      "                                                                 \n",
      " flatten_2 (Flatten)         (None, 12)                0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 1)                 13        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 213\n",
      "Trainable params: 213\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "5f8f0eb2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x241090b96c0>"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## setting verbose = 0 will allow it to run without showing the details while running\n",
    "model.fit(X,y,epochs=10, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "8d95f569",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 98ms/step - loss: 0.6791 - accuracy: 0.6667\n"
     ]
    }
   ],
   "source": [
    "## we can get the accuracy\n",
    "loss, accuracy = model.evaluate(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "ac4ce218",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.04412745, -0.02495219, -0.0116336 , -0.02701507],\n",
       "       [ 0.0139653 , -0.04981779, -0.00911032,  0.01580619],\n",
       "       [-0.04613188,  0.02010217,  0.01894413, -0.01934381],\n",
       "       [-0.0298726 , -0.0473005 ,  0.01380773, -0.00454892],\n",
       "       [ 0.04266732, -0.03342768, -0.03747449, -0.01624268],\n",
       "       [-0.01013995, -0.04050047, -0.04836781, -0.00160445],\n",
       "       [ 0.00233431, -0.04162521, -0.00708358,  0.02677996],\n",
       "       [-0.01186272,  0.00588728,  0.04224113, -0.01820941],\n",
       "       [ 0.03772887,  0.04073342,  0.01848713,  0.01465025],\n",
       "       [-0.01980211, -0.00809649,  0.03347385, -0.02775606],\n",
       "       [ 0.00342171, -0.00703942, -0.0247803 ,  0.03139445],\n",
       "       [-0.00323264, -0.03301443, -0.02449607,  0.03898866],\n",
       "       [-0.0395863 ,  0.05332593,  0.01765579,  0.05746831],\n",
       "       [ 0.01400489, -0.02096782, -0.04555124, -0.02888478],\n",
       "       [ 0.03090746,  0.02640349,  0.00206935,  0.04896965],\n",
       "       [ 0.02403334, -0.04613544, -0.01931006,  0.02950878],\n",
       "       [-0.02046987,  0.01415979, -0.0108415 ,  0.02885196],\n",
       "       [-0.03296052,  0.03326447,  0.04707133,  0.02682573],\n",
       "       [-0.01695817,  0.00768303, -0.019335  , -0.01885702],\n",
       "       [-0.01004438,  0.02840534, -0.00766913,  0.00107106],\n",
       "       [-0.03981604, -0.01312381,  0.02005644, -0.00917243],\n",
       "       [-0.0315247 ,  0.00205199,  0.01064583,  0.03515978],\n",
       "       [ 0.04628324, -0.00925618, -0.01029575,  0.03719496],\n",
       "       [ 0.03514504,  0.03451834, -0.02847688,  0.02560969],\n",
       "       [-0.04460901, -0.02752341, -0.04766246,  0.01607727],\n",
       "       [-0.00344919,  0.00151797, -0.03460795,  0.01095384],\n",
       "       [-0.0143832 ,  0.04439845, -0.01243993,  0.00636022],\n",
       "       [ 0.04445919, -0.03685929,  0.05808485,  0.00325684],\n",
       "       [ 0.04371322, -0.00618447,  0.04179567, -0.02701887],\n",
       "       [ 0.02040935,  0.03059569, -0.00267466, -0.02671039],\n",
       "       [-0.01545327,  0.03396168, -0.04435694, -0.04415436],\n",
       "       [ 0.01588582,  0.05740626, -0.01484738,  0.03387136],\n",
       "       [ 0.03503822,  0.01843127,  0.04145395, -0.01229494],\n",
       "       [-0.03767503,  0.00635729, -0.01430143,  0.0442194 ],\n",
       "       [-0.03361239, -0.00757863,  0.02536016, -0.03020862],\n",
       "       [ 0.01221994, -0.00716194, -0.04681343,  0.00218822],\n",
       "       [-0.05445392, -0.00296866, -0.0038777 , -0.02939718],\n",
       "       [ 0.04031775,  0.00382123, -0.00781958, -0.04837842],\n",
       "       [-0.00195472, -0.01212323,  0.04058229, -0.01416391],\n",
       "       [-0.01202961, -0.0385941 , -0.03658636, -0.00578469],\n",
       "       [ 0.00150491, -0.02176662,  0.00304418, -0.04766929],\n",
       "       [-0.01126981, -0.02543529,  0.04724758, -0.01835628],\n",
       "       [-0.01030246, -0.02778143, -0.01080383,  0.02519867],\n",
       "       [ 0.02391255,  0.01202236, -0.03555449, -0.00288219],\n",
       "       [ 0.04614909,  0.02728048,  0.030596  , -0.03100053],\n",
       "       [ 0.02810067,  0.04208184,  0.03160155,  0.03701687],\n",
       "       [ 0.02935888, -0.03920643,  0.02788493,  0.04224386],\n",
       "       [-0.04554821,  0.02086496, -0.01193884,  0.04101202],\n",
       "       [-0.01527455,  0.03405072, -0.01169766,  0.01729825],\n",
       "       [-0.03517092,  0.00263039,  0.0466742 ,  0.02690727]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### In this example we are interested in embedding not really the classification.\n",
    "### We can get the coefficient (weights) for the embedding (layer). This was why we named the embedding layer. \n",
    "\n",
    "weights = model.get_layer('embedding').get_weights()[0]\n",
    "weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "bd22be42",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(weights) \n",
    "## this gives the vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "8c93277f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.01202961, -0.0385941 , -0.03658636, -0.00578469], dtype=float32)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 'Nice' corresponds to 39\n",
    "weights[39]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "367426f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.0395863 ,  0.05332593,  0.01765579,  0.05746831], dtype=float32)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 'Amazing' corresponds to 12\n",
    "weights[12]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9944ee6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# You would think these coefficients would be the same because the words are similar\n",
    "# But our datatset was very small \n",
    "# If you run it on a huge dataset, maybe you would find these vectors to be similar\n",
    "# and then you can compute the cosine similarity of these two vectors \n",
    "# this gives you an idea of how keras embedding layer works\n",
    "# you can save this to a file and later on load the saved embedding. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
